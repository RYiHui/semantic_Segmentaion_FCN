{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport os\nimport torch as t\nfrom torch.utils.data import Dataset\nimport torchvision.transforms.functional as ff\nfrom PIL import Image\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import DataLoader","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-11-23T05:44:34.585804Z","iopub.execute_input":"2021-11-23T05:44:34.586168Z","iopub.status.idle":"2021-11-23T05:44:35.117710Z","shell.execute_reply.started":"2021-11-23T05:44:34.586071Z","shell.execute_reply":"2021-11-23T05:44:35.116988Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"先验知识：\n    classmethod和staticmethod有什么不同？\n    classmethod必须将类对象的引用作为第一个参数，而staticmethod可以根本没有参数","metadata":{}},{"cell_type":"code","source":"class LabelProcessor:\n    #静态方法的应用场景：在构造对象之前，就先从类中获取到一定的信息来判断如何初始化实例（另一种构造函数）\n    def __init__(self,file_path):\n        self.colormap = self.read_color_map(file_path)\n        self.cm2lbl = self.encode_label_pix(self.colormap)\n    \n    #静态方法不许哟啊self参数\n    #节约内存，不用每个实例都实例化方法\n    #实际和C++的静态函数没有区别，可以直接通过类名来调用\n    @staticmethod\n    def read_color_map(file_path):\n        pd_label_color = pd.read_csv(file_path, sep=',')\n        colormap = []\n        for i in range(len(pd_label_color.index)):\n            # 通过行号索引行数据\n            tmp = pd_label_color.iloc[i]\n            color = []\n            color.append(tmp['r'])\n            color.append(tmp['g'])\n            color.append(tmp['b'])\n            colormap.append(color)\n        return colormap\n    \n\n    @staticmethod\n    def encode_label_pix(colormap):\n        cm2lbl = np.zeros(256 ** 3)\n        for i,cm in enumerate(colormap):\n            cm2lbl[(cm[0] * 256 + cm[1]) * 256 + cm[2]] = i\n        return cm2lbl\n  \n    #按照Label_dict对比找到对应类别序号\n    def encode_label_img(self,img):\n        data = np.array(img, dtype='int32')\n        idx = (data[:, :, 0] * 256 + data[:, :, 1]) * 256 + data[:, :, 2]\n        return np.array(self.cm2lbl[idx], dtype='int64')","metadata":{"execution":{"iopub.status.busy":"2021-11-23T05:44:37.729580Z","iopub.execute_input":"2021-11-23T05:44:37.730260Z","iopub.status.idle":"2021-11-23T05:44:37.742296Z","shell.execute_reply.started":"2021-11-23T05:44:37.730221Z","shell.execute_reply":"2021-11-23T05:44:37.741533Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"class CamvidDataset(Dataset):\n    def __init__(self,file_path=[],crop_size=None):\n        \"\"\"\n            filepath(list):数据和标签路径，列表元素第一个为图片路径，第二个为标签路径\n        \"\"\"\n        #1.正确读入图片和标签路径\n        if len(file_path)!=2:\n            raise ValueError(\"同时需要图片拟合标签文件夹的路径，图片路径在前\")\n        self.img_path = file_path[0]\n        self.label_path = file_path[1]\n        \n        #2. 从路径中取出图片和标签数据的文件名保持到两个列表当中（程序中的数据来源）\n        #文件夹路径->文件列表\n        self.imgs=self.read_file(self.img_path)\n        self.labels=self.read_file(self.label_path)\n        self.crop_size=crop_size\n    \n    def __getitem__(self,index):\n        img = self.imgs[index]\n        label = self.labels[index]\n        img = Image.open(img)\n        label = Image.open(label).convert('RGB')\n        img,label=self.center_crop(img,label,self.crop_size)\n        img,label=self.img_transform(img,label)\n        sample = {'img':img,'label':label}\n        return sample\n                                          \n    def __len__(self):\n        return len(self.imgs)\n                            \n    def read_file(self,path):\n        \"\"\"从文件读取数据\"\"\"\n        file_list=os.listdir(path)\n        file_path_list=[os.path.join(path,img) for img in file_list]            \n        file_path_list.sort()\n        return file_path_list\n                        \n    def center_crop(self,data,label,crop_size):\n        data = ff.center_crop(data, crop_size)\n        label = ff.center_crop(label, crop_size)\n        return data, label                   \n                                        \n    def img_transform(self, img, label):\n        #numpy->tensor  label从RGB图改到类别图\n        label = np.array(label)\n        label = Image.fromarray(label.astype('uint8'))\n\n        transform_img = transforms.Compose(\n            [\n                transforms.ToTensor(),\n                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n            ]\n        )\n        img = transform_img(img)\n        label = label_processor.encode_label_img(label)\n        #1.声明类时这里不会执行，所以不会报错\n        #2.在运行到此处时，label_processor已经被初始化，所以即可使用\n        label = t.from_numpy(label)\n\n        return img, label\n                                          \n                                          \n                                          ","metadata":{"execution":{"iopub.status.busy":"2021-11-23T05:44:41.794678Z","iopub.execute_input":"2021-11-23T05:44:41.795015Z","iopub.status.idle":"2021-11-23T05:44:41.811750Z","shell.execute_reply.started":"2021-11-23T05:44:41.794982Z","shell.execute_reply":"2021-11-23T05:44:41.810906Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"TRAIN_ROOT = '/kaggle/input/camvid/CamVid/train'\nTRAIN_LABEL = '/kaggle/input/camvid/CamVid/train_labels'\nVAL_ROOT = '/kaggle/input/camvid/CamVid/val'\nVAL_LABEL = '/kaggle/input/camvid/CamVid/val_labels'\nTEST_ROOT = '/kaggle/input/camvid/CamVid/test'\nTEST_LABEL = '/kaggle/input/camvid/CamVid/test_labels'\nCLASS_DICT='/kaggle/input/camvid/CamVid/class_dict.csv'\ncrop_size=(352,480)\nCam_train = CamvidDataset([TRAIN_ROOT, TRAIN_LABEL],crop_size)\nCam_val = CamvidDataset([VAL_ROOT, VAL_LABEL],crop_size)\nCam_test = CamvidDataset([TEST_ROOT, TEST_LABEL],crop_size)\nlabel_processor = LabelProcessor(CLASS_DICT)","metadata":{"execution":{"iopub.status.busy":"2021-11-23T05:44:44.428953Z","iopub.execute_input":"2021-11-23T05:44:44.429625Z","iopub.status.idle":"2021-11-23T05:44:44.456042Z","shell.execute_reply.started":"2021-11-23T05:44:44.429587Z","shell.execute_reply":"2021-11-23T05:44:44.455299Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"Example for load data","metadata":{}},{"cell_type":"markdown","source":"Example Image show","metadata":{}},{"cell_type":"code","source":"img = Image.open(Cam_train.imgs[0])\nlabel = Image.open(Cam_train.labels[0])\nplt.subplot(1,2,1)\nplt.imshow(img)\nplt.subplot(1,2,2)\nplt.imshow(label)\n#裁剪前\nimg,label=Cam_train.center_crop(img,label,crop_size)\nplt.subplot(1,2,1)\nplt.imshow(img)\nplt.subplot(1,2,2)\nplt.imshow(label)\n#裁剪之后的结果","metadata":{"execution":{"iopub.status.busy":"2021-11-23T05:44:46.846796Z","iopub.execute_input":"2021-11-23T05:44:46.847224Z","iopub.status.idle":"2021-11-23T05:44:47.343630Z","shell.execute_reply.started":"2021-11-23T05:44:46.847187Z","shell.execute_reply":"2021-11-23T05:44:47.342981Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def bilinear_kernel(in_channels, out_channels, kernel_size):\n    \"\"\"Define a bilinear kernel according to in channels and out channels.\n    Returns:\n        return a bilinear filter tensor\n    \"\"\"\n    factor = (kernel_size + 1) // 2\n    if kernel_size % 2 == 1:\n        center = factor - 1\n    else:\n        center = factor - 0.5\n    og = np.ogrid[:kernel_size, :kernel_size]\n    bilinear_filter = (1 - abs(og[0] - center) / factor) * (1 - abs(og[1] - center) / factor)\n    weight = np.zeros((in_channels, out_channels, kernel_size, kernel_size), dtype=np.float32)\n    weight[range(in_channels), range(out_channels), :, :] = bilinear_filter\n    return t.from_numpy(weight)","metadata":{"_kg_hide-output":false,"execution":{"iopub.status.busy":"2021-11-23T05:44:49.170171Z","iopub.execute_input":"2021-11-23T05:44:49.170748Z","iopub.status.idle":"2021-11-23T05:44:49.178259Z","shell.execute_reply.started":"2021-11-23T05:44:49.170710Z","shell.execute_reply":"2021-11-23T05:44:49.177536Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"from torchvision import models\nfrom torch import nn\npretrained_net = models.vgg16_bn(pretrained=False)","metadata":{"execution":{"iopub.status.busy":"2021-11-23T05:44:50.454189Z","iopub.execute_input":"2021-11-23T05:44:50.455012Z","iopub.status.idle":"2021-11-23T05:44:52.668893Z","shell.execute_reply.started":"2021-11-23T05:44:50.454975Z","shell.execute_reply":"2021-11-23T05:44:52.668114Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"Upsample上采样只能把图片尺寸变大，而不能把通道数还愿为小通道，所以需要另写1**1卷积来缩小通道数 conv_trans1","metadata":{}},{"cell_type":"code","source":"#pretrained_net.features\n#-----------model-------------------------- ","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-23T04:09:19.856726Z","iopub.execute_input":"2021-11-23T04:09:19.857354Z","iopub.status.idle":"2021-11-23T04:09:19.860864Z","shell.execute_reply.started":"2021-11-23T04:09:19.857307Z","shell.execute_reply":"2021-11-23T04:09:19.859896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FCN(nn.Module):\n    def __init__(self,num_classes):\n        super().__init__()\n        \n        self.stage1 = pretrained_net.features[:7]\n        self.stage2 = pretrained_net.features[7:14]\n        self.stage3 = pretrained_net.features[14:24]\n        self.stage4 = pretrained_net.features[24:34]\n        self.stage5 = pretrained_net.features[34:]\n        \n        #降维\n        self.scores1 = nn.Conv2d(512,num_classes,1)  #input_channel out_channel kernel_size\n        self.scores2 = nn.Conv2d(512,num_classes,1)  \n        self.scores3 = nn.Conv2d(128,num_classes,1)\n        \n        self.conv_trans1 = nn.Conv2d(512,256,1)\n        self.conv_trans2 = nn.Conv2d(256,num_classes,1)\n        \n        self.upsample_8x = nn.ConvTranspose2d(num_classes, num_classes, 16, 8, 4, bias=False)\n        self.upsample_8x.weight.data = bilinear_kernel(num_classes, num_classes, 16)\n\n        self.upsample_2x_1 = nn.ConvTranspose2d(512, 512, 4, 2, 1, bias=False)\n        self.upsample_2x_1.weight.data = bilinear_kernel(512, 512, 4)\n\n        self.upsample_2x_2 = nn.ConvTranspose2d(256, 256, 4, 2, 1, bias=False)\n        self.upsample_2x_2.weight.data = bilinear_kernel(256, 256, 4)\n        \n    def forward(self,x):     #352 480  3\n        s1 = self.stage1(x)   #176 240  64 \n        s2 = self.stage2(s1)  #88  120 128\n        s3 = self.stage3(s2)  #44  60  256  \n        s4 = self.stage4(s3)  #22  30  512\n        s5 = self.stage5(s4)  #11  15  512\n        \n    \n        s5=self.upsample_2x_1(s5)  #22 30 512  放大\n        add1=s4+s5   \n        \n     \n        add1=self.conv_trans1(add1)   # 22 30 256 先改深度再改大小\n        add1=self.upsample_2x_2(add1)  #44 60 256\n        add2=add1+s3\n        \n        add2=self.conv_trans2(add2) #22 30 12\n        output=self.upsample_8x(add2) #352 480 12\n        return output\n        ","metadata":{"execution":{"iopub.status.busy":"2021-11-23T05:45:10.181158Z","iopub.execute_input":"2021-11-23T05:45:10.181838Z","iopub.status.idle":"2021-11-23T05:45:10.193249Z","shell.execute_reply.started":"2021-11-23T05:45:10.181803Z","shell.execute_reply":"2021-11-23T05:45:10.192591Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"from __future__ import division\n\nimport six  #python2 -python3\ndef calc_semantic_segmentation_confusion(pred_labels, gt_labels):\n    pred_labels = iter(pred_labels)  #(352 480)\n\n    gt_labels = iter(gt_labels)  #(352 480)\n    #初始化\n    n_class = 12\n    confusion = np.zeros((n_class, n_class), dtype=np.int64)\n    #构造\n    for pred_label, gt_label in six.moves.zip(pred_labels, gt_labels):\n        if pred_label.ndim != 2 or gt_label.ndim != 2:\n            raise ValueError('ndim of labels should be two.')\n        if pred_label.shape != gt_label.shape:\n            raise ValueError('Shape of ground truth and prediction should'\n                             ' be same.')\n        pred_label = pred_label.flatten()  #(168960,)\n        gt_label = gt_label.flatten()  #(168960,)\n\n        # Dynamically expand the confusion matrix if necessary.\n        lb_max = np.max((pred_label, gt_label))\n        # print(lb_max)\n        if lb_max >= n_class:\n            expanded_confusion = np.zeros(\n                (lb_max + 1, lb_max + 1), dtype=np.int64)\n            expanded_confusion[0:n_class, 0:n_class] = confusion\n\n            n_class = lb_max + 1\n            confusion = expanded_confusion\n\n        # Count statistics from valid pixels.  极度巧妙 × class_nums 正好使得每个ij能够对应.\n        mask = gt_label >= 0\n        confusion += np.bincount(\n            n_class * gt_label[mask].astype(int) +\n            pred_label[mask], minlength=n_class ** 2).reshape((n_class, n_class))\n\n    for iter_ in (pred_labels, gt_labels):\n        # This code assumes any iterator does not contain None as its items.\n        if next(iter_, None) is not None:\n            raise ValueError('Length of input iterables need to be same')\n\n    # confusion = np.delete(confusion, 11, axis=0)\n    # confusion = np.delete(confusion, 11, axis=1)\n    return confusion\n\n\ndef calc_semantic_segmentation_iou(confusion):\n   \n    \n    iou_denominator = (confusion.sum(axis=1) + confusion.sum(axis=0)\n                       - np.diag(confusion))\n    iou = np.diag(confusion) / iou_denominator\n    return iou[:-1]\n    # return iou\n\n\ndef eval_semantic_segmentation(pred_labels, gt_labels):\n  \n    confusion = calc_semantic_segmentation_confusion(\n        pred_labels, gt_labels)\n    iou = calc_semantic_segmentation_iou(confusion)\n    pixel_accuracy = np.diag(confusion).sum() / confusion.sum()\n    class_accuracy = np.diag(confusion) / (np.sum(confusion, axis=1) + 1e-10)\n\n    return {'iou': iou, 'miou': np.nanmean(iou),\n            'pixel_accuracy': pixel_accuracy,\n            'class_accuracy': class_accuracy,\n            'mean_class_accuracy': np.nanmean(class_accuracy[:-1])}\n            # 'mean_class_accuracy': np.nanmean(class_accuracy)}","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-23T05:45:15.749364Z","iopub.execute_input":"2021-11-23T05:45:15.749894Z","iopub.status.idle":"2021-11-23T05:45:15.764490Z","shell.execute_reply.started":"2021-11-23T05:45:15.749856Z","shell.execute_reply":"2021-11-23T05:45:15.762380Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"import torch.nn.functional as F\nfrom torch import optim \nfrom torch.autograd import Variable\nfrom datetime import datetime\n\n#1.data\ntrain_data = DataLoader(Cam_train,batch_size=2,shuffle=True,num_workers=4)\nval_data = DataLoader(Cam_val,batch_size=2,shuffle=True,num_workers=4)\n\n#2.equires\ndevice = t.device('cuda')  if t.cuda.is_available() else t.device('cpu')\nnet = FCN(32)\nnet = net.to(device)\ncriterion = nn.NLLLoss().to(device)\noptimizer = optim.Adam(net.parameters(),lr=1e-4)\n\n\neval_miou_list = []\nbest = [0]","metadata":{"execution":{"iopub.status.busy":"2021-11-23T06:51:13.142007Z","iopub.execute_input":"2021-11-23T06:51:13.142901Z","iopub.status.idle":"2021-11-23T06:51:13.216921Z","shell.execute_reply.started":"2021-11-23T06:51:13.142858Z","shell.execute_reply":"2021-11-23T06:51:13.216206Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"------------------Train---------------------------","metadata":{}},{"cell_type":"code","source":"#500个epoch\nprec_time=datetime.now()\nfor epoch in range(50):\n    if epoch%10==0 and epoch!=0:\n        for group in optimizer.param_groups:\n            group['lr']*=0.25\n    train_loss = 0  #一个epoch的所有损失和\n    train_acc = 0 \n    train_miou = 0\n    \n    #open \n    net=net.train()\n   \n    \n    #i,sample一个epoch\n    for i, sample in enumerate(train_data):\n        \n        imgdata = Variable(sample['img'].to(device))\n        imglabel = Variable(sample['label'].long().to(device))\n\n        optimizer.zero_grad()\n        out = net(imgdata)\n        out = F.log_softmax(out, dim=1)\n\n        loss = criterion(out, imglabel)\n\n        loss.backward()\n        optimizer.step()\n        train_loss = loss.item() + train_loss\n\n        pre_label = out.max(dim=1)[1].data.cpu().numpy()\n        pre_label = [i for i in pre_label]\n\n        true_label = imglabel.data.cpu().numpy()\n        true_label = [i for i in true_label]\n\n        eval_metrix = eval_semantic_segmentation(pre_label, true_label)\n        train_acc = eval_metrix['mean_class_accuracy'] + train_acc\n        train_miou = eval_metrix['miou'] + train_miou\n        \n    net = net.eval()\n    eval_loss = 0\n    eval_acc = 0\n    eval_miou = 0\n    eval_class_acc = 0\n    #一个验证机epoch\n    for j, sample in enumerate(val_data):\n        valImg = Variable(sample['img'].to(device))\n        valLabel = Variable(sample['label'].long().to(device))\n\n        out = net(valImg)\n        out = F.log_softmax(out, dim=1)\n        loss = criterion(out, valLabel)\n        eval_loss = loss.item() + eval_loss\n        pre_label = out.max(dim=1)[1].data.cpu().numpy()\n        pre_label = [i for i in pre_label]\n\n        true_label = valLabel.data.cpu().numpy()\n        true_label = [i for i in true_label]\n\n        eval_metrics = eval_semantic_segmentation(pre_label, true_label)\n        eval_acc = eval_metrics['mean_class_accuracy'] + eval_acc\n        eval_miou = eval_metrics['miou'] + eval_miou\n    \n    cur_time = datetime.now()\n    h, remainder = divmod((cur_time - prec_time).seconds, 3600)\n    m, s = divmod(remainder, 60)\n    #LOG PRINT------------------------------------------------\n    epoch_str = ('|Epoch|: {}\\n|Train Loss|: {:.5f}\\n|Train Acc|: {:.5f}\\n|Train Mean IU|: {:.5f}\\n'\n                '|Valid Loss|: {:.5f}\\n|Valid Acc|: {:.5f}\\n|Valid Mean IU|: {:.5f}\\n'.format(\n                epoch, train_loss / len(train_data), train_acc / len(train_data), train_miou / len(train_data)\n                ,eval_loss / len(train_data), eval_acc/len(val_data),\n                eval_miou/len(val_data)))\n    time_str = 'Time: {:.0f}:{:.0f}:{:.0f}'.format(h, m, s)\n    print(epoch_str + \"\\n\" + time_str)\n    #SAVE------------------------------------------------------\n    #eval_miou越大越好\n    if (max(best) <= eval_miou/len(val_data)):\n        best.append(eval_miou/len(val_data))\n        t.save(net.state_dict(),  'version1.pth')","metadata":{"_kg_hide-output":false,"scrolled":true,"execution":{"iopub.status.busy":"2021-11-23T04:09:23.102025Z","iopub.execute_input":"2021-11-23T04:09:23.102289Z","iopub.status.idle":"2021-11-23T04:48:37.525018Z","shell.execute_reply.started":"2021-11-23T04:09:23.102256Z","shell.execute_reply":"2021-11-23T04:48:37.524176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"----------------------------TEST--------------------------------------","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ls","metadata":{"execution":{"iopub.status.busy":"2021-11-23T04:48:37.526952Z","iopub.execute_input":"2021-11-23T04:48:37.527501Z","iopub.status.idle":"2021-11-23T04:48:38.237188Z","shell.execute_reply.started":"2021-11-23T04:48:37.52745Z","shell.execute_reply":"2021-11-23T04:48:38.236345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch as t\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader\n\n\ndevice = t.device('cuda') if t.cuda.is_available() else t.device('cpu')\n\nBATCH_SIZE = 4\nmiou_list = [0]\ntest_data = DataLoader(Cam_test, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\nnet = FCN(32)\nnet.eval()\nnet.to(device)\nnet.load_state_dict(t.load('version1.pth'))\n\ntrain_acc = 0\ntrain_miou = 0\ntrain_class_acc = 0\ntrain_mpa = 0\nerror = 0\n\nfor i, sample in enumerate(test_data):\n\tdata = Variable(sample['img']).to(device)\n\tlabel = Variable(sample['label']).to(device)\n\tout = net(data)\n\tout = F.log_softmax(out, dim=1)\n\n\tpre_label = out.max(dim=1)[1].data.cpu().numpy()\n\tpre_label = [i for i in pre_label]\n\n\ttrue_label = label.data.cpu().numpy()\n\ttrue_label = [i for i in true_label]\n\n\teval_metrix = eval_semantic_segmentation(pre_label, true_label)\n\ttrain_acc = eval_metrix['mean_class_accuracy'] + train_acc\n\ttrain_miou = eval_metrix['miou'] + train_miou\n\ttrain_mpa = eval_metrix['pixel_accuracy'] + train_mpa\n\tif len(eval_metrix['class_accuracy']) < 12:\n\t\teval_metrix['class_accuracy'] = 0\n\t\ttrain_class_acc = train_class_acc + eval_metrix['class_accuracy']\n\t\terror += 1\n\telse:\n\t\ttrain_class_acc = train_class_acc + eval_metrix['class_accuracy']\n\n\tprint(eval_metrix['class_accuracy'], '================', i)\n\n\nepoch_str = ('test_acc :{:.5f} ,test_miou:{:.5f}, test_mpa:{:.5f}, test_class_acc :{:}'.format(train_acc /(len(test_data)-error),\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\ttrain_miou/(len(test_data)-error), train_mpa/(len(test_data)-error),\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\ttrain_class_acc/(len(test_data)-error)))\n\nif train_miou/(len(test_data)-error) > max(miou_list):\n\tmiou_list.append(train_miou/(len(test_data)-error))\n\tprint(epoch_str+'==========last')","metadata":{"execution":{"iopub.status.busy":"2021-11-23T06:51:16.345025Z","iopub.execute_input":"2021-11-23T06:51:16.345293Z","iopub.status.idle":"2021-11-23T06:51:26.293542Z","shell.execute_reply.started":"2021-11-23T06:51:16.345265Z","shell.execute_reply":"2021-11-23T06:51:26.292687Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"Predict","metadata":{}},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2021-11-23T06:50:45.449170Z","iopub.execute_input":"2021-11-23T06:50:45.449626Z","iopub.status.idle":"2021-11-23T06:50:46.142191Z","shell.execute_reply.started":"2021-11-23T06:50:45.449587Z","shell.execute_reply":"2021-11-23T06:50:46.141376Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"device = t.device('cuda') if t.cuda.is_available() else t.device('cpu')\n\ntest_data = DataLoader(Cam_test, batch_size=1, shuffle=True, num_workers=4)\n\nnet = FCN(32).to(device)\nnet.load_state_dict(t.load(\"version1.pth\"))\nnet.eval()\n\npd_label_color = pd.read_csv('/kaggle/input/camvid/CamVid/class_dict.csv', sep=',')\nname_value = pd_label_color['name'].values\nnum_class = len(name_value)\ncolormap = []\nfor i in range(num_class):\n\ttmp = pd_label_color.iloc[i]\n\tcolor = []\n\tcolor.append(tmp['r'])\n\tcolor.append(tmp['g'])\n\tcolor.append(tmp['b'])\n\tcolormap.append(color)\n\ncm = np.array(colormap).astype('uint8')\n\ndir = \"/kaggle/working/output/\"\n\nfor i, sample in enumerate(test_data):\n\tvalImg = sample['img'].to(device)\n\tvalLabel = sample['label'].long().to(device)\n\tout = net(valImg)\n\tout = F.log_softmax(out, dim=1)\n\tpre_label = out.max(1)[1].squeeze().cpu().data.numpy()\n\tpre = cm[pre_label]\n\tpre1 = Image.fromarray(pre)\n\tpre1.save(dir + str(i) + '.png')\n\tprint('Done')","metadata":{"execution":{"iopub.status.busy":"2021-11-23T06:51:26.295636Z","iopub.execute_input":"2021-11-23T06:51:26.296117Z","iopub.status.idle":"2021-11-23T06:51:45.665413Z","shell.execute_reply.started":"2021-11-23T06:51:26.296074Z","shell.execute_reply":"2021-11-23T06:51:45.664458Z"},"trusted":true},"execution_count":32,"outputs":[]}]}